import torch
import json
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel


LLM_MODEL_NAME = "meta-llama/Llama-2-7b-hf"
LORA_ADAPTER_PATH = "./models/finetuned_adapters/aym_raportor_adapter"  # VerÅŸler elde edilince buraya gelicek
CONTEXT_WINDOW_SIZE = 4096
MAX_NEW_TOKENS = 1024
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

#
class MockVectorDB:
    def __init__(self):
        # Sahte emsal kararlar (retrieval sonucunu simÃ¼le eder)
        self.mock_docs = [
            "Emsal Karar 1 (YÃ¼ksek Mahkeme KararÄ±, 2018/123): Bir vatandaÅŸÄ±n adil yargÄ±lanma hakkÄ± (Anayasa Md. 36) ihlali talebinde, delil toplama sÃ¼recindeki usul hatalarÄ± temel gerekÃ§e olarak kabul edilmiÅŸtir.",
            "Emsal Karar 2 (BÃ¶lge Mahkemesi KararÄ±, 2020/456): Hakimin reddi talebinin reddedilmesi, somut olayda tarafsÄ±zlÄ±k ÅŸÃ¼phesi yaratmadÄ±ÄŸÄ±ndan, Anayasa'nÄ±n ihlal edilmediÄŸi sonucuna varÄ±lmÄ±ÅŸtÄ±r.",
            "Emsal Karar 3 (AYM KararÄ±, 2022/789): Ã–zel hayatÄ±n gizliliÄŸi (Anayasa Md. 20) ihlali iddialarÄ±nda, kamu yararÄ± ve orantÄ±lÄ±lÄ±k ilkesinin gÃ¶zetilmesi gerektiÄŸi vurgulanmÄ±ÅŸtÄ±r."
        ]

    def query(self, query: str, k: int = 3) -> list:
        
        
        print(f"\n[RAG] VektÃ¶r DB sorgusu yapÄ±lÄ±yor: '{query[:50]}...'")
        time.sleep(0.5) # SimÃ¼lasyon gecikmesi
        return self.mock_docs[:k]


def load_llm_and_tokenizer():
    
    print(f"\n[LLM] Temel model ({LLM_MODEL_NAME}) yÃ¼kleniyor...")
    
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_NAME,
        quantization_config=bnb_config,
        device_map=DEVICE,
        max_length=CONTEXT_WINDOW_SIZE
    )
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
 
    if False: 
        print(f"[LLM] LoRA AdaptÃ¶rÃ¼ ({LORA_ADAPTER_PATH}) yÃ¼kleniyor...")
        model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
        # Modelin adaptÃ¶rle birleÅŸtirilmesi:
        # model = model.merge_and_unload() 
        print("[LLM] AYM RaportÃ¶r uzmanlÄ±ÄŸÄ± baÅŸarÄ±yla entegre edildi.")
    else:
        print("[LLM] DÄ°KKAT: Model adaptÃ¶rsÃ¼z (generic) modda Ã§alÄ±ÅŸÄ±yor.")

    return model, tokenizer


def create_full_prompt(system_prompt: str, retrieved_context: str, user_query: str) -> str:
    
    
    
    SYSTEM_PROMPT = (
        "Sen, TÃ¼rkiye Cumhuriyeti AnayasasÄ± ve emsal kararlar konusunda uzman, deneyimli bir Anayasa Mahkemesi raportÃ¶rÃ¼sÃ¼n. "
        "GÃ¶revin, sana verilen Olay Ã–zeti (BaÅŸvuru), Ä°lgili Anayasa Maddesi ve Emsal KararlarÄ± inceleyerek, bu hukuki Ã¶ncÃ¼llere dayanan "
        "tutarlÄ± bir GerekÃ§eli Karar (Justified Decision) taslaÄŸÄ± oluÅŸturmaktÄ±r. "
        "Ã‡Ä±ktÄ±n, sadece aÅŸaÄŸÄ±daki formatÄ± (TALEP, Ä°NCELEME, HUKUKÄ° DEÄžERLENDÄ°RME, HÃœKÃœM) iÃ§ermelidir."
    )
    
    
    CONTEXT_BLOCK = f"""
--- BAÄžLAM BÄ°LGÄ°SÄ° (EMSALLER) ---
{retrieved_context}
---
"""
    
    
    CASE_DETAILS = f"""
*** BAÅžVURU DETAYLARI ***
{user_query}
"""
    
    
    FULL_PROMPT = f"{SYSTEM_PROMPT}\n{CONTEXT_BLOCK}\n{CASE_DETAILS}"
    
    return FULL_PROMPT


def generate_decision(model, tokenizer, query: str):
    
    
    
    vector_db = MockVectorDB()
    retrieved_docs = vector_db.query(query, k=3)
    context_text = "\n".join([f"- {doc}" for doc in retrieved_docs])
    
   
    full_prompt = create_full_prompt(
        system_prompt=None, 
        retrieved_context=context_text,
        user_query=query
    )
    
    
    print(f"\n[GENERATION] LLM (Generic) yanÄ±t Ã¼retiyor...")
    
    inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True).to(DEVICE)
    
    
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id
        )

    
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    response = generated_text.replace(full_prompt, "").strip()
    
    
    if "HÃœKÃœM" not in response:
        response = "[UYARI: HENÃœZ FINE-TUNING YAPILMADIÄžI Ä°Ã‡Ä°N FORMAT KUSURLU OLABÄ°LÄ°R.]\n" + response

    return response, context_text

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    
    llm_model, llm_tokenizer = load_llm_and_tokenizer()
    
    
    example_query = (
        "Olay Ã–zeti: Ä°stanbul'da bir avukatÄ±n ofisine, mahkeme kararÄ± olmaksÄ±zÄ±n polis tarafÄ±ndan girilerek arama yapÄ±lmÄ±ÅŸ ve bazÄ± belgelere el konulmuÅŸtur. "
        "Ä°hlal Ä°ddiasÄ±: Konut DokunulmazlÄ±ÄŸÄ± (Anayasa Md. 21) ve Ã–zel HayatÄ±n GizliliÄŸi (Anayasa Md. 20)."
    )
    
   
    final_decision, used_context = generate_decision(llm_model, llm_tokenizer, example_query)

    print("\n" + "="*80)
    print("ðŸŽ¯ AYM RAPORTÃ–RÃœ TASLAK KARARI (Weeks 3 & 4 Ã‡Ä±ktÄ±sÄ± - GENERIC MOD)")
    print("="*80)
    print("\n[KULLANILAN EMSAL BAÄžLAMI]:")
    print(used_context)
    print("\n" + "-"*40)
    print("\n[ ÃœRETÄ°LEN KARAR TASLAÄžI]:")
    print(final_decision)
    print("\n" + "="*80)
    
